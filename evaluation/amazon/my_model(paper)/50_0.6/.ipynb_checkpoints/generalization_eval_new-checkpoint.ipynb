{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "output_path = '/DATA/joosung/controllable_english/evaluation/amazon/ST_v1.0/50_0.6/amazon/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  joo1_50_06\n",
      "BLEU score: 45.471442259594205\n",
      "model:  joo2_50_06\n",
      "BLEU score: 45.4879295064028\n",
      "model:  joo3_50_06\n",
      "BLEU score: 47.67541229474618\n",
      "model:  joo4_50_06\n",
      "BLEU score: 48.28828444102569\n",
      "model:  joo5_50_06\n",
      "BLEU score: 48.87648471242795\n",
      "model:  joo6_50_06\n",
      "BLEU score: 47.40203823277322\n"
     ]
    }
   ],
   "source": [
    "### self-BLEU\n",
    "import glob\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_bleu_1 = 0\n",
    "    neg_bleu_2 = 0\n",
    "    neg_bleu_3 = 0\n",
    "    neg_bleu_4 = 0\n",
    "    neg_bleu_score = 0\n",
    "    for k in range(neg_len):\n",
    "        ori_sen = neg_data_dataset[k].split('\\t')[0]\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1]\n",
    "        \n",
    "        ori_tokens=word_tokenize(ori_sen)\n",
    "        out_tokens=word_tokenize(out_sen)\n",
    "        ori_tokens = [ori_tokens]\n",
    "        \n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        \n",
    "        neg_bleu_1 += bleu_1_score\n",
    "        neg_bleu_2 += bleu_2_score\n",
    "        neg_bleu_3 += bleu_3_score\n",
    "        neg_bleu_4 += bleu_4_score\n",
    "        neg_bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_bleu_1 = 0\n",
    "    pos_bleu_2 = 0\n",
    "    pos_bleu_3 = 0\n",
    "    pos_bleu_4 = 0\n",
    "    pos_bleu_score = 0\n",
    "    for k in range(pos_len):\n",
    "        ori_sen = pos_data_dataset[k].split('\\t')[0]\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1]\n",
    "        \n",
    "        ori_tokens=word_tokenize(ori_sen)\n",
    "        out_tokens=word_tokenize(out_sen)\n",
    "        ori_tokens = [ori_tokens]\n",
    "        \n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        \n",
    "        pos_bleu_1 += bleu_1_score\n",
    "        pos_bleu_2 += bleu_2_score\n",
    "        pos_bleu_3 += bleu_3_score\n",
    "        pos_bleu_4 += bleu_4_score\n",
    "        pos_bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))\n",
    "    \n",
    "    bleu_1 = (neg_bleu_1+pos_bleu_1)/(neg_len+pos_len)\n",
    "    bleu_2 = (neg_bleu_2+pos_bleu_2)/(neg_len+pos_len)\n",
    "    bleu_3 = (neg_bleu_3+pos_bleu_3)/(neg_len+pos_len)\n",
    "    bleu_4 = (neg_bleu_4+pos_bleu_4)/(neg_len+pos_len)\n",
    "    blue_score = (neg_bleu_score+pos_bleu_score)/(neg_len+pos_len)\n",
    "#     print('BLEU_1 score: {}'.format(bleu_1))\n",
    "#     print('BLEU_2 score: {}'.format(bleu_2))\n",
    "#     print('BLEU_3 score: {}'.format(bleu_3))\n",
    "#     print('BLEU_4 score: {}'.format(bleu_4))\n",
    "#     print('Average: ', (bleu_1+bleu_2+bleu_3+bleu_4)/4*100)\n",
    "    print('BLEU score: {}'.format(blue_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  joo1_50_06\n",
      "BLEU score: 20.337338497327302\n",
      "model:  joo2_50_06\n",
      "BLEU score: 20.65775554813196\n",
      "model:  joo3_50_06\n",
      "BLEU score: 21.846110160471408\n",
      "model:  joo4_50_06\n",
      "BLEU score: 22.134538743058076\n",
      "model:  joo5_50_06\n",
      "BLEU score: 22.18165919257615\n",
      "model:  joo6_50_06\n",
      "BLEU score: 21.349529918713518\n"
     ]
    }
   ],
   "source": [
    "### human-BLEU\n",
    "import glob\n",
    "human_path = '/DATA/joosung/controllable_english/evaluation/amazon/compare1/amazon/'\n",
    "neg_human = human_path + 'sentiment.test.0.human'\n",
    "neg_human_open = open(neg_human, \"r\")\n",
    "neg_human_dataset = neg_human_open.readlines()\n",
    "neg_human_open.close()\n",
    "\n",
    "pos_human = human_path + 'sentiment.test.1.human'\n",
    "pos_human_open = open(pos_human, \"r\")\n",
    "pos_human_dataset = pos_human_open.readlines()\n",
    "pos_human_open.close()\n",
    "\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_bleu_score = 0\n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1]\n",
    "        out_tokens=word_tokenize(out_sen)\n",
    "        \n",
    "        ### human\n",
    "        ori_sen = neg_human_dataset[k].split('\\t')[1]        \n",
    "        ori_tokens=word_tokenize(ori_sen)        \n",
    "        ori_tokens = [ori_tokens]\n",
    "        \n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        \n",
    "        neg_bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_bleu_score = 0\n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1]\n",
    "        out_tokens=word_tokenize(out_sen)\n",
    "\n",
    "        ### human\n",
    "        ori_sen = pos_human_dataset[k].split('\\t')[1]        \n",
    "        ori_tokens=word_tokenize(ori_sen)        \n",
    "        ori_tokens = [ori_tokens]\n",
    "        \n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        \n",
    "        pos_bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))\n",
    "    \n",
    "    blue_score = (neg_bleu_score+pos_bleu_score)/(neg_len+pos_len)\n",
    "    print('BLEU score: {}'.format(blue_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from transformers import *\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "import json\n",
    "f = open('../../amazon_vocab.json')\n",
    "token2num = json.load(f)\n",
    "\n",
    "num2token = {}\n",
    "for key, value in token2num.items():\n",
    "    num2token[value] = key\n",
    "import sys    \n",
    "sys.path.insert(0, \"/DATA/joosung/controllable_english/amazon/visual_v1_1/\")\n",
    "from dis_model import *\n",
    "dismodel = findattribute().cuda()\n",
    "\n",
    "i='5'\n",
    "dismodel_name='cls_model_' + str(i)\n",
    "dismodel.load_state_dict(torch.load('/DATA/joosung/controllable_english/amazon/visual_v1_1/models/{}'.format(dismodel_name)))\n",
    "dismodel.eval()\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  joo1_50_06\n",
      "Accuracy: 66.5%\n",
      "model:  joo2_50_06\n",
      "Accuracy: 65.3%\n",
      "model:  joo3_50_06\n",
      "Accuracy: 64.3%\n",
      "model:  joo4_50_06\n",
      "Accuracy: 63.3%\n",
      "model:  joo5_50_06\n",
      "Accuracy: 63.9%\n",
      "model:  joo6_50_06\n",
      "Accuracy: 63.5%\n"
     ]
    }
   ],
   "source": [
    "### classifier accuracy\n",
    "import glob\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_correct = 0\n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1].strip()\n",
    "        \n",
    "        token_idx = torch.tensor(gpt_tokenizer.encode(out_sen)).unsqueeze(0).cuda()\n",
    "\n",
    "        \"\"\"discriminator\"\"\"\n",
    "        if token_idx.shape[1] != 0:\n",
    "            result = dismodel.discriminator(token_idx=token_idx).argmax(1).cpu().numpy()[0]    \n",
    "            if result == 1: ## style transfer so result must be 1(positive)\n",
    "                neg_correct += 1\n",
    "            \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_correct = 0\n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1].strip()\n",
    "        \n",
    "        token_idx = torch.tensor(gpt_tokenizer.encode(out_sen)).unsqueeze(0).cuda()\n",
    "\n",
    "        \"\"\"discriminator\"\"\"\n",
    "        if token_idx.shape[1] != 0:\n",
    "            result = dismodel.discriminator(token_idx=token_idx).argmax(1).cpu().numpy()[0]    \n",
    "            if result == 0: ## style transfer so result must be 0(negative)\n",
    "                pos_correct += 1\n",
    "    \n",
    "    Acc = (neg_correct+pos_correct)/(neg_len+pos_len)*100\n",
    "    print(\"Accuracy: {}%\".format(Acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f9f2bb82c74cfaa7290794c6ffcb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49336f511b7a4824b921e9cc87978b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.98 seconds, 251.22 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f433013dc06d4bbfb95cb97c0a09ee6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d0bd4cfb914034bf018038e63b8539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 4.03 seconds, 248.24 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71b98758a9c4edab8d895e431417f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f200dd2b297e44fe818814786f44844e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.82 seconds, 261.62 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb40e2f233f4f9fb13eda90914b38fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf038d5baa74713a5802563d32f5518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.66 seconds, 273.39 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c2616c5cda4a50a6cd6f3be7c7907e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72dc30d45df4a58a123defce797067b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.06 seconds, 326.44 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd63e064435942dc8400f1d8bea27d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62884f13af594428851e2f0071211937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 4.13 seconds, 242.22 sentences/sec\n",
      "model:  joo1_50_06\n",
      "BERT score(F1): 89.17235696911811\n",
      "\n",
      "model:  joo2_50_06\n",
      "BERT score(F1): 89.34523285031318\n",
      "\n",
      "model:  joo3_50_06\n",
      "BERT score(F1): 89.51758585572243\n",
      "\n",
      "model:  joo4_50_06\n",
      "BERT score(F1): 89.49587355256081\n",
      "\n",
      "model:  joo5_50_06\n",
      "BERT score(F1): 89.78166531324386\n",
      "\n",
      "model:  joo6_50_06\n",
      "BERT score(F1): 89.40161507725716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### BERT score with human reference\n",
    "from bert_score import score\n",
    "import glob\n",
    "human_path = '/DATA/joosung/controllable_english/evaluation/amazon/compare1/amazon/'\n",
    "neg_human = human_path + 'sentiment.test.0.human'\n",
    "neg_human_open = open(neg_human, \"r\")\n",
    "neg_human_dataset = neg_human_open.readlines()\n",
    "neg_human_open.close()\n",
    "\n",
    "pos_human = human_path + 'sentiment.test.1.human'\n",
    "pos_human_open = open(pos_human, \"r\")\n",
    "pos_human_dataset = pos_human_open.readlines()\n",
    "pos_human_open.close()\n",
    "\n",
    "human_refs = []\n",
    "neg_len = len(neg_human_dataset)\n",
    "pos_len = len(pos_human_dataset)\n",
    "for k in range(neg_len):\n",
    "    ref_sen = neg_human_dataset[k].split('\\t')[1]\n",
    "    human_refs.append(ref_sen)\n",
    "for k in range(pos_len):\n",
    "    ref_sen = pos_human_dataset[k].split('\\t')[1]\n",
    "    human_refs.append(ref_sen)\n",
    "    \n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "model_name=[]\n",
    "score_list=[]\n",
    "for i in range(len(neg_out_files)):    \n",
    "    cands = []\n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1]\n",
    "        cands.append(out_sen)\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1]\n",
    "        cands.append(out_sen)\n",
    "\n",
    "    P, R, human_F1 = score(cands, human_refs, lang='en', verbose=True)\n",
    "    human_F1_list=list(human_F1.numpy())\n",
    "    human_BERT_scroe = sum(human_F1_list)/len(human_F1_list)\n",
    "    \n",
    "    model_name.append(neg_out_files[i].split('.')[-1])\n",
    "    score_list.append(human_BERT_scroe)\n",
    "for i in range(len(model_name)):\n",
    "    print(\"model: \", model_name[i])\n",
    "    print('BERT score(F1): {}'.format(score_list[i]*100))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea90314927cd4230a201d040bd2cd92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5e642b0c5d4976b403bb4b4a75a92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.80 seconds, 263.44 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4ca6f5a11448caa3d2dc0cd0c47629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7990d8292147496da2fe8ea433b3c0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.89 seconds, 256.90 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206782c1b9a94039a8d42776c9c0cc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78bd4f6699074722bcad5c357389ecb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.77 seconds, 264.97 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87902c4784664f4682c282337c5957cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5deb00284845818a7f8e0445087b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.89 seconds, 257.25 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58de7a7f7b9141639596e6e7fbe7ec34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4ab3b67c2d4caf8de4d78f495c0048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.88 seconds, 257.83 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8263fc85e540cfbd054e6218e6bc87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9201c3883db54e0d865096a861ec1ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 4.11 seconds, 243.45 sentences/sec\n",
      "model:  joo1_50_06\n",
      "BERT score(F1): 91.75102634429932\n",
      "\n",
      "model:  joo2_50_06\n",
      "BERT score(F1): 91.92863015532492\n",
      "\n",
      "model:  joo3_50_06\n",
      "BERT score(F1): 92.1997271001339\n",
      "\n",
      "model:  joo4_50_06\n",
      "BERT score(F1): 92.23236696720123\n",
      "\n",
      "model:  joo5_50_06\n",
      "BERT score(F1): 92.52344387173652\n",
      "\n",
      "model:  joo6_50_06\n",
      "BERT score(F1): 92.08147238492965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### BERT score with self-reference\n",
    "from bert_score import score\n",
    "import glob\n",
    "\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "\n",
    "neg_human = neg_out_files[0]\n",
    "neg_human_open = open(neg_human, \"r\")\n",
    "neg_human_dataset = neg_human_open.readlines()\n",
    "neg_human_open.close()\n",
    "\n",
    "pos_human = pos_out_files[0]\n",
    "pos_human_open = open(pos_human, \"r\")\n",
    "pos_human_dataset = pos_human_open.readlines()\n",
    "pos_human_open.close()\n",
    "\n",
    "refs = []\n",
    "neg_len = len(neg_human_dataset)\n",
    "pos_len = len(pos_human_dataset)\n",
    "for k in range(neg_len):\n",
    "    ref_sen = neg_human_dataset[k].split('\\t')[0]\n",
    "    refs.append(ref_sen)\n",
    "for k in range(pos_len):\n",
    "    ref_sen = pos_human_dataset[k].split('\\t')[0]\n",
    "    refs.append(ref_sen)\n",
    "        \n",
    "model_name=[]\n",
    "score_list=[]    \n",
    "for i in range(len(neg_out_files)):\n",
    "    cands = []\n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1]\n",
    "        cands.append(out_sen)\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1]\n",
    "        cands.append(out_sen)\n",
    "\n",
    "    P, R, F1 = score(cands, refs, lang='en', verbose=True)\n",
    "    F1_list=list(F1.numpy())\n",
    "    BERT_score = sum(F1_list)/len(F1_list)\n",
    "    model_name.append(neg_out_files[i].split('.')[-1])\n",
    "    score_list.append(BERT_score)\n",
    "for i in range(len(model_name)):\n",
    "    print(\"model: \", model_name[i])\n",
    "    print('BERT score(F1): {}'.format(score_list[i]*100))\n",
    "    print('')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PPL\n",
    "import torch, os\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device='cpu'\n",
    "model_class, tokenizer_class = (GPT2LMHeadModel, GPT2Tokenizer)\n",
    "gpt2_lm_tokenizer = tokenizer_class.from_pretrained('gpt2-large')\n",
    "gpt2_lm_model = model_class.from_pretrained('gpt2-large')\n",
    "gpt2_lm_model.to(device)\n",
    "gpt2_lm_model.eval()\n",
    "\n",
    "lm_loss = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def calculate_ppl_gpt(sentence_batch):\n",
    "    # tokenize the sentences\n",
    "    tokenized_ids = [None for i in range(len(sentence_batch))]\n",
    "    \n",
    "    for i in range(len(sentence_batch)):\n",
    "        tokenized_ids[i] = gpt2_lm_tokenizer.encode(sentence_batch[i])\n",
    "        \n",
    "    sen_lengths = [len(x) for x in tokenized_ids]\n",
    "    max_sen_lenght = max(sen_lengths)\n",
    "    \n",
    "    n_batch = len(sentence_batch)\n",
    "    input_ids = np.zeros( shape=(n_batch, max_sen_lenght), dtype=np.int64)\n",
    "    lm_labels = np.full(shape=(n_batch, max_sen_lenght), fill_value=-1)\n",
    "    \n",
    "    for i, tokens in enumerate(tokenized_ids):\n",
    "        input_ids[i, :len(tokens)] = tokens\n",
    "        lm_labels[i, :len(tokens)-1] = tokens[1:] \n",
    "    \n",
    "    input_ids = torch.tensor(input_ids).to(device)\n",
    "    lm_labels = torch.tensor(lm_labels).to(device)\n",
    "    with torch.no_grad():\n",
    "        lm_pred = gpt2_lm_model(input_ids)\n",
    "    loss_val = lm_loss(lm_pred[0].view(-1, lm_pred[0].size(-1)), lm_labels.view(-1))\n",
    "    normalized_loss = loss_val.view(n_batch,-1).sum(dim= -1) / torch.tensor(sen_lengths, dtype=torch.float32).to(device)\n",
    "    #normalized_loss = loss_val.view(n_batch,-1).sum(dim= -1)\n",
    "    ppl = torch.exp(normalized_loss)\n",
    "    return  ppl.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  joo1_50_06\n",
      "PPL: 367.7347538342476\n",
      "model:  joo2_50_06\n",
      "PPL: 380.0693036046028\n",
      "model:  joo3_50_06\n",
      "PPL: 390.83086039114\n",
      "model:  joo4_50_06\n",
      "PPL: 452.2586282913685\n",
      "model:  joo5_50_06\n",
      "PPL: 474.19696709299086\n",
      "model:  joo6_50_06\n",
      "PPL: 461.5459375874996\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_PPL = 0\n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1].strip()\n",
    "        if len(out_sen) != 0:\n",
    "            sample_PPL = calculate_ppl_gpt([out_sen])[0]\n",
    "            neg_PPL += sample_PPL\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_PPL = 0\n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1].strip()\n",
    "        if len(out_sen) != 0:\n",
    "            sample_PPL = calculate_ppl_gpt([out_sen])[0]\n",
    "            pos_PPL += sample_PPL\n",
    "\n",
    "    total_PPL = (neg_PPL+pos_PPL)/(neg_len+pos_len)\n",
    "    print('PPL: {}'.format(total_PPL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "### PPL\n",
    "import torch, os\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "print(device)\n",
    "\n",
    "lm_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "lm_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "path = '/DATA/joosung/controllable_english/gpt2/finetune_amazon/final/pytorch_model.bin'\n",
    "lm_model_state_dict = torch.load(path)\n",
    "lm_model.load_state_dict(lm_model_state_dict)\n",
    "lm_model.to(device)\n",
    "lm_model.eval()\n",
    "\n",
    "lm_loss = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def calculate_ppl_gpt_yelp(sentence_batch):\n",
    "    # tokenize the sentences\n",
    "    tokenized_ids = [None for i in range(len(sentence_batch))]\n",
    "    \n",
    "    for i in range(len(sentence_batch)):\n",
    "        tokenized_ids[i] = lm_tokenizer.encode(sentence_batch[i])\n",
    "        \n",
    "    sen_lengths = [len(x) for x in tokenized_ids]\n",
    "    max_sen_lenght = max(sen_lengths)\n",
    "    \n",
    "    n_batch = len(sentence_batch)\n",
    "    input_ids = np.zeros( shape=(n_batch, max_sen_lenght), dtype=np.int64)\n",
    "    lm_labels = np.full(shape=(n_batch, max_sen_lenght), fill_value=-1)\n",
    "    \n",
    "    for i, tokens in enumerate(tokenized_ids):\n",
    "        input_ids[i, :len(tokens)] = tokens\n",
    "        lm_labels[i, :len(tokens)-1] = tokens[1:] \n",
    "    \n",
    "    input_ids = torch.tensor(input_ids).to(device)\n",
    "    lm_labels = torch.tensor(lm_labels).to(device)\n",
    "    with torch.no_grad():\n",
    "        lm_pred = lm_model(input_ids)\n",
    "    loss_val = lm_loss(lm_pred[0].view(-1, lm_pred[0].size(-1)), lm_labels.view(-1))\n",
    "    normalized_loss = loss_val.view(n_batch,-1).sum(dim= -1) / torch.tensor(sen_lengths, dtype=torch.float32).to(device)\n",
    "    #normalized_loss = loss_val.view(n_batch,-1).sum(dim= -1)\n",
    "    ppl = torch.exp(normalized_loss)\n",
    "    return  ppl.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  joo1_50_06\n",
      "PPL: 4.513803875803948\n",
      "model:  joo2_50_06\n",
      "PPL: 29.1247275185585\n",
      "model:  joo3_50_06\n",
      "PPL: 8.846343151450156\n",
      "model:  joo4_50_06\n",
      "PPL: 14.671672335147857\n",
      "model:  joo5_50_06\n",
      "PPL: 12.950323242545128\n",
      "model:  joo6_50_06\n",
      "PPL: 3.793945886418956e+16\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_PPL = 0\n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1].strip()\n",
    "        if len(out_sen) != 0:\n",
    "            sample_PPL = calculate_ppl_gpt_yelp([out_sen])[0]\n",
    "            neg_PPL += sample_PPL\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_PPL = 0\n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1].strip()\n",
    "        if len(out_sen) != 0:\n",
    "            sample_PPL = calculate_ppl_gpt_yelp([out_sen])[0]\n",
    "            pos_PPL += sample_PPL\n",
    "\n",
    "    total_PPL = (neg_PPL+pos_PPL)/(neg_len+pos_len)\n",
    "    print('PPL: {}'.format(total_PPL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
